{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f8ed995-6ce8-42f9-8592-bbb7fc9d9ec6",
   "metadata": {},
   "source": [
    "# Changelog and Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b5de9f-2d14-4a4a-8fd9-44fe08baf28d",
   "metadata": {},
   "source": [
    "### Changelog\n",
    "**Modifying file processing and creation (22/02/2024):**\n",
    "* Made separate files for training_clean, training_pneu, testing_clean and testing_pneu in the BuildData process. This should make it easier to split data evenly when training the model (i.e. we can choose an even split between \"clean\" and \"pneu\" samples).\n",
    "\n",
    "**Fixing data splitting and the running network (24/02/2024):**\n",
    "* Split files evenly. Created dataset \"testing_both\" and \"training_both\" as required, as well as their children datasets. Fixed issues with unshuffled data.\n",
    "* Fixed \"train_X\" being bigger than \"train_y\" by reading images with cv2.IMREAD_GRAYSCALE to reduce dimensionality.\n",
    "* Ran neural network locally. Tweaked learning rate and other parameters to prevent neuron death (this was an issue).\n",
    "* *Current results (saved in \"pneumonia_model2\")*:  92.7% on test_X.\n",
    "\n",
    "**Changed path names for generalization (29/02/2024)**\n",
    "* Modified implementation of \"Building Datasets\" (slightly) to include generalized path names.\n",
    "* Included assert statements (very briefly) and some error-handling to avoid misuse.\n",
    "* Made project ready for initial public release."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a01925b-adf7-433b-900d-81d89861a69b",
   "metadata": {},
   "source": [
    "### For next time:\n",
    "* Optimize the network by modifying parameters: learning rate, image compression size (IMG_SIZE), number of layers and neurons.\n",
    "* Try implementing momentum and/or decaying learning rate. Maybe use SGD instead of Adam for optimization. How do things change?\n",
    "* Run CNN on data that is UNMODIFIED (namely, go back to the Chest-X dataset and find the other datasets; see generalizations).\n",
    "\n",
    "Note: try implementing all of this remotely; use cloud computing software to run these larger batches and epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc03c2cf-5c88-4ccf-9ccd-76f54d148e78",
   "metadata": {},
   "source": [
    "### Research Ideas:\n",
    "1. Compare learning rate decay methods in optimizing results (eg. \"None\" vs. \"ReduceLROnPlateau\"). Which one is best here?\n",
    "2. Does the IMG_SIZE affect results much? To what extent can this be reduced and results be preserved?\n",
    "3. Visualizing intermediate layers as images (after all, these are convolutional layers, so they should be visualizable)\n",
    "4. Creating a GAN to artificially create images of lungs. How realistic can these images be? (I'll have to make another network to differentiate between pictures of lungs and outliers, so this is another kind of deep learning application). Then run these through the existing network to see what it says about pneumonia/clean. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51095f8b-362d-4b40-8cca-b3b8bfe97f7c",
   "metadata": {},
   "source": [
    "# Implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c23210-037e-46d5-bc4b-1e78bfa39bd0",
   "metadata": {},
   "source": [
    "### Building Datasets:\n",
    "**BEFORE RUNNING:**\n",
    "Ensure the dataset found at the following link is installed, and paths are configured as specified in the next cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "903a9a44-99a5-4396-8e5a-283eef39b22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting relevant constants.\n",
    "\n",
    "# An absolute path to the current directory that \"public_pneumonia.ipynb\" is currently in: \n",
    "#    ~~ Modify before executing anything else in \"public_pneumonia.ipynb\"! ~~\n",
    "ROOT_DIR = None\n",
    "\n",
    "# An absolute path to the directory \"Chest X-Rays.v3-augmented.folder\":\n",
    "#    ~~ Modify before executing anything else in \"public_pneumonia.ipynb\"! ~~\n",
    "DATA_DIR = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b9bd192d-1bce-4c95-9bb0-dd92affa7ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Please update DATA_DIR in the above cell (TypeError at BuildData: __init__()).\n",
      "ERROR: Please update ROOT_DIR in the above cell (TypeError at BuildData: make_data()).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "\n",
    "# Constants to modify based on data being built:\n",
    "REBUILD_TRAIN_DATA = False\n",
    "REBUILD_TEST_DATA = True\n",
    "\n",
    "\n",
    "class BuildData():\n",
    "    # Chosen image size for compression:\n",
    "    IMG_SIZE = 100\n",
    "    # Initializing datasets:\n",
    "    build_clean = []\n",
    "    build_pneu = []\n",
    "    datasets = [build_clean, build_pneu]\n",
    "    # Counting instances:\n",
    "    cleancount = 0\n",
    "    pneucount = 0\n",
    "    \n",
    "    def __init__(self, data_type):\n",
    "        # Universal paths to each of the unprocessed datasets:\n",
    "        self.data_type = data_type\n",
    "        try:\n",
    "            self.LUNGS_CLEAN = DATA_DIR + f\"/{data_type}/NORMAL\"\n",
    "            self.LUNGS_PNEU = DATA_DIR + f\"/{data_type}/PNEUMONIA\"\n",
    "        except TypeError:\n",
    "            print(\"ERROR: Please update DATA_DIR in the above cell (TypeError at BuildData: __init__()).\")\n",
    "        # Enumerating values for each path in a dictionary:\n",
    "        self.LABELS = {self.LUNGS_CLEAN: 0, self.LUNGS_PNEU: 1}\n",
    "        \n",
    "\n",
    "    # Reading, processing and collecting data into build_data:\n",
    "    #    type:    \"train\"/\"test\"\n",
    "    def make_data(self):\n",
    "        # Iterating between CLEAN and PNEU directories, and saving respectively:\n",
    "        for label in self.LABELS:\n",
    "            print(label)\n",
    "            for f in tqdm(os.listdir(label)):\n",
    "                try:\n",
    "                    path = os.path.join(label, f)  # creating full path to image\n",
    "                    # Loading/reading the image:\n",
    "                    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "                    img = cv2.resize(img, (self.IMG_SIZE, self.IMG_SIZE))  # image resize\n",
    "                    \n",
    "                    # Appending to appropriate dataset:\n",
    "                    if label == self.LUNGS_CLEAN:   # label is clean\n",
    "                        self.build_clean.append([np.array(img), np.eye(2)[self.LABELS[label]]])\n",
    "                        self.cleancount += 1\n",
    "                    elif label == self.LUNGS_PNEU:                               # label is pneu\n",
    "                        self.build_pneu.append([np.array(img), np.eye(2)[self.LABELS[label]]])\n",
    "                        self.pneucount += 1\n",
    "    \n",
    "                except Exception as e:\n",
    "                    pass\n",
    "                \n",
    "        # Shuffling our data randomly:\n",
    "        np.random.shuffle(self.build_clean)\n",
    "        np.random.shuffle(self.build_pneu)\n",
    "\n",
    "        # Saving to the appropriate files:\n",
    "        filename_clean = ROOT_DIR + f\"processed/{self.data_type}ing_clean.pkl\"\n",
    "        filename_pneu = ROOT_DIR + f\"processed/{self.data_type}ing_pneu.pkl\"\n",
    "        \n",
    "        with open(filename_clean, \"wb\") as f:\n",
    "            pickle.dump(self.build_clean, f)\n",
    "            print(f\"Saved CLEAN scans '{self.data_type}' in {filename_clean}.\")\n",
    "        with open(filename_pneu, \"wb\") as f:\n",
    "            pickle.dump(self.build_pneu, f)\n",
    "            print(f\"Saved PNEU scans '{self.data_type}' in {filename_pneu}.\")\n",
    "\n",
    "        print(f\"Clean count: {self.cleancount}\")\n",
    "        print(f\"Pneu count: {self.pneucount}\")\n",
    "\n",
    "\n",
    "if REBUILD_TRAIN_DATA:\n",
    "    try:\n",
    "        pneuvsclean = BuildData(\"train\")\n",
    "        pneuvsclean.make_data()\n",
    "    except TypeError:\n",
    "        print(\"ERROR: Please update ROOT_DIR in the above cell (TypeError at BuildData: make_data()).\")\n",
    "\n",
    "if REBUILD_TEST_DATA:\n",
    "    try: \n",
    "        pneuvsclean = BuildData(\"test\")\n",
    "        pneuvsclean.make_data()\n",
    "    except:\n",
    "        print(\"ERROR: Please update ROOT_DIR in the above cell (TypeError at BuildData: make_data()).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f4d96cd-59c1-4ede-a364-dc7c65b49951",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "ERROR: ROOT_DIR is NoneType.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Loading all data from files:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m ROOT_DIR \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR: ROOT_DIR is NoneType.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m DATA_DIR \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR: DATA_DIR is NoneType.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m training_clean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mROOT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/processed/training_clean.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: ERROR: ROOT_DIR is NoneType."
     ]
    }
   ],
   "source": [
    "assert ROOT_DIR != None, \"ERROR: ROOT_DIR is NoneType.\"\n",
    "assert DATA_DIR != None, \"ERROR: DATA_DIR is NoneType.\"\n",
    "\n",
    "# Loading all data from files:\n",
    "\n",
    "training_clean = np.load(f\"{ROOT_DIR}/processed/training_clean.pkl\", allow_pickle=True)\n",
    "training_pneu = np.load(f\"{ROOT_DIR}/processed/training_pneu.pkl\", allow_pickle=True)\n",
    "testing_clean = np.load(f\"{ROOT_DIR}/processed/testing_clean.pkl\", allow_pickle=True)\n",
    "testing_pneu = np.load(f\"{ROOT_DIR}/processed/testing_pneu.pkl\", allow_pickle=True)\n",
    "\n",
    "# Displaying first element graphically using pyplot, for intuition:\n",
    "plt.imshow(training_clean[3][0])\n",
    "plt.show()\n",
    "\n",
    "# [1, 0]:  Clean\n",
    "# [0, 1]:  Pneu\n",
    "print(training_clean[1][1])\n",
    "\n",
    "\n",
    "# Displaying first element graphically using pyplot, for intuition:\n",
    "plt.imshow(training_pneu[3][0])\n",
    "plt.show()\n",
    "\n",
    "# [1, 0]:  Clean\n",
    "# [0, 1]:  Pneu\n",
    "print(training_pneu[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e483b0-c9c3-4778-8297-6071873dc14a",
   "metadata": {},
   "source": [
    "### Splitting Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edca87c4-8982-46a8-b412-7898c04c2daa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "ERROR: ROOT_DIR is NoneType.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m ROOT_DIR \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR: ROOT_DIR is NoneType.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m DATA_DIR \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR: DATA_DIR is NoneType.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# The loading datasets code is run again:\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Loading all data from files:\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: ERROR: ROOT_DIR is NoneType."
     ]
    }
   ],
   "source": [
    "assert ROOT_DIR != None, \"ERROR: ROOT_DIR is NoneType.\"\n",
    "assert DATA_DIR != None, \"ERROR: DATA_DIR is NoneType.\"\n",
    "\n",
    "################################################################################\n",
    "# The loading datasets code is run again:\n",
    "# Loading all data from files:\n",
    "training_clean = np.load(f\"{ROOT_DIR}/processed/training_clean.pkl\", allow_pickle=True)\n",
    "training_pneu = np.load(f\"{ROOT_DIR}/processed/training_pneu.pkl\", allow_pickle=True)\n",
    "testing_clean = np.load(f\"{ROOT_DIR}/processed/testing_clean.pkl\", allow_pickle=True)\n",
    "testing_pneu = np.load(f\"{ROOT_DIR}/processed/testing_pneu.pkl\", allow_pickle=True)\n",
    "\n",
    "################################################################################\n",
    "\n",
    "\n",
    "# How many samples of each (CLEAN and PNEU) is desired?\n",
    "TRAIN_SPLIT_INPUT = 500\n",
    "# Capping train_split by its limiting factors:\n",
    "TRAIN_SPLIT = min(TRAIN_SPLIT_INPUT, min(len(training_clean), len(training_pneu)))\n",
    "\n",
    "# Shuffling existing datasets to ensure randomness:\n",
    "np.random.shuffle(training_clean)\n",
    "np.random.shuffle(training_pneu)\n",
    "\n",
    "np.random.shuffle(testing_clean)\n",
    "np.random.shuffle(testing_pneu)\n",
    "\n",
    "# Concatenating lists and shuffling result:\n",
    "training_both = training_clean[:TRAIN_SPLIT] + training_pneu[:TRAIN_SPLIT]\n",
    "testing_both = testing_clean + testing_pneu\n",
    "\n",
    "np.random.shuffle(training_both)\n",
    "np.random.shuffle(testing_both)\n",
    "\n",
    "# Conversion to a Tensor:\n",
    "#    (scaled to avoid overflow)\n",
    "train_X = torch.Tensor(np.array([np.array(i[0]) for i in training_both])).view(-1, 100, 100) / 255\n",
    "train_y = torch.Tensor(np.array([np.array(i[1]) for i in training_both]))\n",
    "\n",
    "test_X = torch.Tensor(np.array([np.array(i[0]) for i in testing_both])).view(-1, 100, 100) / 255\n",
    "test_y = torch.Tensor(np.array([np.array(i[1]) for i in testing_both]))\n",
    "\n",
    "# Checking the lengths are equal:\n",
    "print(len(train_X))\n",
    "print(len(train_y))\n",
    "\n",
    "print(len(test_X))\n",
    "print(len(test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80729006-9f8b-46e7-95e3-59ccbb117b81",
   "metadata": {},
   "source": [
    "### Creating Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "2f965635-4067-48cb-8d89-5f4a862bb323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4645, 0.5355]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "CONV_LAYERS = 3\n",
    "LINEAR_LAYERS = 2\n",
    "\n",
    "FLATTEN_SHAPE = 12800\n",
    "\n",
    "# Defining our Neural Network class:\n",
    "#    inherits from nn.Module.\n",
    "class NeuralNet(nn.Module):\n",
    "    # Initialization of a network:\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Convolutional layers:\n",
    "        #    syntax:  nn.Conv2d(inputs, outputs, kernel-size)  for a 2D convnet.\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "        # Defining our flattening function for \"x\":\n",
    "        self.flat = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(FLATTEN_SHAPE, 256)\n",
    "        self.fc2 = nn.Linear(256, 2)\n",
    "\n",
    "    # Forward propagation:\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
    "        # Flattening to pass through linear layers:\n",
    "        x = x.view(-1, FLATTEN_SHAPE)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "        return x\n",
    "\n",
    "net = NeuralNet()\n",
    "x = torch.randn(100, 100).view(-1, 1, 100, 100)\n",
    "net.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877d05dc-7057-491b-9624-2f2252d9545c",
   "metadata": {},
   "source": [
    "#### Standard Adam Optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "3383378a-5a7e-49d3-805b-45065443b070",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 67/67 [00:04<00:00, 13.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0233, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 67/67 [00:04<00:00, 14.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0084, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 67/67 [00:04<00:00, 14.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0045, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Using the Adam optimizer:\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "# Defining loss function MSE:\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "\n",
    "# Running the neural network:\n",
    "# Choosing batch size:\n",
    "BATCH_SIZE = 64\n",
    "BATCHES = len(train_X) // BATCH_SIZE\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in tqdm(range(0, len(train_X), BATCHES)):\n",
    "        batch_X = train_X[i : i + BATCHES].view(-1, 1, 100, 100)\n",
    "        batch_y = train_y[i : i + BATCHES]\n",
    "\n",
    "        # Zeroing gradient:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculating forward propagation:\n",
    "        outputs = net(batch_X)\n",
    "        loss = loss_function(outputs, batch_y)\n",
    "\n",
    "        # Backpropagation:\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b24c91f-248f-46f9-890b-2b1d71fc65b7",
   "metadata": {},
   "source": [
    "### Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "3ec56069-6c84-4e21-8b39-4ca977a0ab64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:01<00:00, 356.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9\n",
      "Total: 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Testing accuracy:\n",
    "correct = 0\n",
    "total = 0\n",
    "TEST_RANGE = 400\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(TEST_RANGE)):\n",
    "        real_class = torch.argmax(test_y[i])\n",
    "        net_out = net(test_X[i].view(-1, 1, 100, 100))[0]\n",
    "        predicted_class = torch.argmax(net_out)\n",
    "        if predicted_class == real_class:\n",
    "            correct += 1\n",
    "            # print(f\"Correct! Predicted: {predicted_class}. Actual: {real_class}\")\n",
    "        # else:\n",
    "            # print(f\"WRONG! Predicted: {predicted_class}. Actual: {real_class}\")\n",
    "        total += 1\n",
    "    print(f\"Accuracy: {round(correct/total, 3)}\")\n",
    "    print(f\"Total: {total}\")\n",
    "\n",
    "# plt.imshow(test_X[0])\n",
    "# plt.show()\n",
    "\n",
    "# [1, 0]:  Clean\n",
    "# [0, 1]:  Pneu\n",
    "# print(test_y[0])\n",
    "\n",
    "# Save code for models, if wanted.\n",
    "# torch.save(net.state_dict(), f\"{ROOT_DIR}saved_models/current_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9352e1ce-a9f9-4fe8-ba5c-3e2ef02d6b20",
   "metadata": {},
   "source": [
    "#### Accuracies:\n",
    "\n",
    "* 88.00:  BATCH_SIZE=32, EPOCHS=5\n",
    "\n",
    "* 90.25:  BATCH_SIZE=32, EPOCHS=5\n",
    "* 92.70:  BATCH_SIZE=64, EPOCHS=3  (rerun)\n",
    "* 92.50:  BATCH_SIZE=32, EPOCHS=5  (rerun)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1704c9a8-b06b-4d5d-9ce0-b48dcbad4cae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
